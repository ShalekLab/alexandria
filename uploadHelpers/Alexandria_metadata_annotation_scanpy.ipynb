{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyquist/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import convert_adata_to_scp as conv\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import widget_helpers as uh\n",
    "import MetadataAdder as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexandria metadata annotation for scanpy or Seurat files\n",
    "\n",
    "For this notebook, you are expected to have the following inputs:\n",
    "\n",
    "\n",
    "* ONE AnnData (scanpy) object saved as an h5ad file with the following attributes: \n",
    "    * containing few enough cells that you are willing to save it in dense format\n",
    "    * with some dimensionality reduction already run (ex. UMAP or tSNE)\n",
    "    * with some metadata/clustering already done\n",
    "    * at least one metadata column with a value describing the sample of each cell such that all cells from that sample have an identical value in that column\n",
    "    \n",
    "    \n",
    "---OR---\n",
    "\n",
    "* ONE Seurat .RData object containing:\n",
    "    * ScaleData containing gene expression\n",
    "    * seurat_obs@meta.data containing cell-level metadata\n",
    "    * dimensionality reduction saved in the Seurat object\n",
    "\n",
    "\n",
    "---AND---\n",
    "* (optional) a per sample csv, txt, excel, or tsv file with the following attributes:\n",
    "    * one row per sample\n",
    "    * the values in at least one column are sample names, and these sample names exactly match the values in the sample column in the scanpy/seurat metadata\n",
    "    \n",
    "* (optional) a per donor csv, txt, excel, or tsv file with the following attributes:\n",
    "    * This is useful if you have multiple samples per donor and some metadata that is the same each sample from the same donor\n",
    "    * there should be a column in this file with values as donor identifiers that maps to a column in the sample csv or the scanpy/seurat object\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## Troubleshooting:\n",
    "\n",
    "If you are not seeing graphical output try running ``jupyter labextension install @jupyter-widgets/jupyterlab-manager``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER INPUT: Paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path to anndata/scanpy object, leave as \"\" if you are not using it\n",
    "anndata_path = \"\"#\"../../epithelial_cell_clustering.h5ad\" #TODO allow for multiple objects!\n",
    "\n",
    "# path to sample file, leave as \"\" if you are not using it\n",
    "sample_metadata_path = \"\"\n",
    "\n",
    "# path to donor file, leave as \"\" if you are not using it\n",
    "donor_metadata_path = \"\"\n",
    "\n",
    "# TODO: NOT SUPPORTED YET\n",
    "# paths to cell level metadata files, leave as [] if you are not using it - this is for if you happened to save your cell metadata files separately\n",
    "# these should be: cells as rows and metadata/clusters as columns\n",
    "cell_metadata_paths = []\n",
    "\n",
    "# path to .Rds seurat object, leave as \"\" if you are not using it\n",
    "seurat_paths = [\"/Users/nyquist/Dropbox (MIT)/Shalek Lab (Team folder conflict)/Projects/Gates/NIH.Vaccine.Route.Comparison.2019/Objects/Week25.All.Seurat.Rdata\",\n",
    "              \"/Users/nyquist/Dropbox (MIT)/Shalek Lab (Team folder conflict)/Projects/Gates/NIH.Vaccine.Route.Comparison.2019/Objects/Week13.All.Seurat.Rdata\"] #TODO allow for multiple objects!\n",
    "\n",
    "# desired output directory full path (leave as \"\" if you want to output to the directory containing this notebook)\n",
    "output_dir = \"\"\n",
    "\n",
    "output_file_names = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif len(cell_metadata_paths) > 0:\\n    if \"cell level dataframe\" in mapping_options:# if you already have other cell level info here\\n        for p in cell_metadata_paths:\\n            if p.split(\".\")[-1] ==\"csv\":\\n                some_cell_level_metadata = pd.read_csv(p, index_col=0)\\n            else: # assume if it is not csv, it is tab-deliminated\\n                some_cell_level_metadata = pd.read_csv(p, index_col=0, sep=\"\\t\")\\n                \\n            if len(set(some_cell_level_metadata.index).intersection(set(mapping_options[\"cell level dataframe\"].index))) < len(some_cell_level_metadata.index):\\n                print(\"Some new cells are not in the existing cell level metadata!! This is probably not going to work!\")\\n            # you are going to overwrite the metadata columns in your other file if they are the same as the ones in this file\\n            for col in some_cell_level_metadata.columns:\\n                mapping_options[\"cell level dataframe\"][col] = \"\"\\n                mapping_options[\"cell level dataframe\"].loc[some_cell_level_metadata.index, ]\\n    if len(cell_metadata_paths) > 1: # you\\'re doing different stuff if you need to merge these or if there is just one\\n        mapping_options[\"cell level files\"] = cell_metadata_paths\\n    elif : # if you already have other cell level info here\\n        # if \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no editing required for this cell, just run it\n",
    "mapping_options = {}\n",
    "if len(anndata_path) > 0:\n",
    "    adata = sc.read_h5ad(anndata_path)\n",
    "    print(adata)\n",
    "    mapping_options[\"anndata\"] = adata\n",
    "    mapping_options[\"cell level dataframe\"] = adata.obs\n",
    "if len(sample_metadata_path) > 0:\n",
    "    mapping_options[\"sample_metadata_path\"] = sample_metadata_path\n",
    "    \n",
    "if len(donor_metadata_path) > 0:\n",
    "    mapping_options[\"donor_metadata_path\"] = donor_metadata_path\n",
    "\n",
    "if len(seurat_paths)  > 0:\n",
    "    %load_ext rpy2.ipython\n",
    "    \n",
    "    # TODO - use rpy2 to read out metadata from seurat, hopefully make a function that accepts as input the name of this feature so this works for several versions of seurat\n",
    "    #seurat_metadata = conv.get_metadata_from_seurat(seurat_paths, \"meta.data\")\n",
    "    # TODO: because reading in seurat files a bunch of times might be way too slow, it could make sense for us to have one function that will open the file, save the expression and cluster files, and return the dataframe\n",
    "    #mapping_options[\"cell level dataframe\"] = seurat_metadata\n",
    "    \n",
    "'''\n",
    "if len(cell_metadata_paths) > 0:\n",
    "    if \"cell level dataframe\" in mapping_options:# if you already have other cell level info here\n",
    "        for p in cell_metadata_paths:\n",
    "            if p.split(\".\")[-1] ==\"csv\":\n",
    "                some_cell_level_metadata = pd.read_csv(p, index_col=0)\n",
    "            else: # assume if it is not csv, it is tab-deliminated\n",
    "                some_cell_level_metadata = pd.read_csv(p, index_col=0, sep=\"\\t\")\n",
    "                \n",
    "            if len(set(some_cell_level_metadata.index).intersection(set(mapping_options[\"cell level dataframe\"].index))) < len(some_cell_level_metadata.index):\n",
    "                print(\"Some new cells are not in the existing cell level metadata!! This is probably not going to work!\")\n",
    "            # you are going to overwrite the metadata columns in your other file if they are the same as the ones in this file\n",
    "            for col in some_cell_level_metadata.columns:\n",
    "                mapping_options[\"cell level dataframe\"][col] = \"\"\n",
    "                mapping_options[\"cell level dataframe\"].loc[some_cell_level_metadata.index, ]\n",
    "    if len(cell_metadata_paths) > 1: # you're doing different stuff if you need to merge these or if there is just one\n",
    "        mapping_options[\"cell level files\"] = cell_metadata_paths\n",
    "    elif : # if you already have other cell level info here\n",
    "        # if \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from R:\n",
    "\n",
    "Now we go into R and convert to text files. The 'scp_save_seruat.R' will take as input the path or paths to seurat objects saved as .Rdata files. Unfortunately, Seurat does not maintain backwards compatibility when it comes to the seurat object data structure. Since everyone is using different versions of seruat, this code might require quite a bit of editing from you. It currently makes the following assumptions:\n",
    "\n",
    "\n",
    "- EXPRESSION FILES: The expression value of interest is stored in seurat.object@data\n",
    "\n",
    "- DIMENSIONALITY REDUCTION: The tsne or umap values are stored in seurat.object@meta.data as columns named 'X_umap1', 'X_umap2', 'X_tsne1', 'X_tsne2' - this script will only automatically check for those two dimensionality reductions. If you have other ones, you will need to g\n",
    "\n",
    "- METADATA: The metadata is saved in seurat.object@meta.data - if there are columns that mean the same thing but have different names in your different seurat objects (ex. in object 1 you called the cell type column \"celltype\" and in object 2 you called the celltype column \"cell.type\") It will be best that you merge those. You will need to write your own code to do this\n",
    "        \n",
    "\n",
    "### IMPORTANT: This series of code blocks will automatically write files with your expression data and dimensionality reduction (cluster) results but not your metadata. This is expected to be passed back to python for annotation into the Alexandria format. Make sure you end up getting this file!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i seurat_paths -i output_dir\n",
    "# Only run this if you data is in seurat\n",
    "# If your objects are huge, you are going to have to do this outside of the notebook\n",
    "source(\"scp_save_seurat.R\")\n",
    "# in case you have cells in your various objects that happen to have the same name, we add a prefix to the cell names.\n",
    "# this array is the prefix to your cell names which should correspond to the order in your 'seurat_paths' variable\n",
    "# keep this an array even if you only have one cell name\n",
    "\n",
    "proj_names <- c(\"Week25\",\"Week13\") #EDIT THIS\n",
    "output_prefix<-paste(output_dir, \"SCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your seurat objects meet all of the assumptions described above, run the following block:\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- expression stored in seurat.object@data\n",
    "\n",
    "- tsne or umap stored in seurat.object@meta.data as 'X_umap1' etc\n",
    "\n",
    "- metadata is in seurat.object@meta.data and you only have one object or you don't mind dealing with misaligned column names later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o merged_metadata \n",
    "# Run the line below if your seurat objects meet all the requirements above and you don't want to merge your metadata manually (or you only have one seurat object)\n",
    "merged_metadata <- load_multiple_seurat_files(seurat_paths, proj_name= proj_names, output_prefix=output_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the above cell, skip down to 'OPTIONAL: Fix misaligned metadata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seurat object step 1: Save expression files\n",
    "\n",
    "If you have more than one seurat object, you will need to do this for each one. You can uncomment the for loop or just copy and paste the code a few times. Beware that the notebook might crash if you load in too much data, so it might be good to go through this whole process once per object (as long as you are saving your metadata as different variable names for each object)\n",
    "\n",
    "\n",
    "### Changes from user:\n",
    "\n",
    "If your data is not saved in your seurat object as seurat.object@data, you will need to change the 'exp.data' variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R  \n",
    "i <- 1  # i is the index in the array 'seurat_paths'\n",
    "#for(i in 1:length(seurat_paths)){\n",
    "obj_<-load(seurat_paths[[path]])\n",
    "obj <- get(obj_)\n",
    "proj_name <- proj_names[i]\n",
    "#    dim_red_types <- c(\"tsne\", \"umap\") # if your dimensionality reduction is not saved in the format described in the text above this cell, remove the values from this array and run dimensionality reduction manually\n",
    "# USER INPUT: if your data is not saved in obj@data, run the following chunk (replacing obj@data with the correct thing)\n",
    "exp.data <-obj@data\n",
    "\n",
    "exp_filename <- paste(output_prefix, \"_norm_expression.txt.gz\",sep=\"\")\n",
    "exp_df <- add_gene_column(exp.data,proj_name) \n",
    "write.csv.gz(x=exp_df, file=exp_filename, quote = FALSE,sep = \"\\t\",col.names = TRUE)\n",
    "\n",
    "#}\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seurat object step 2: Save cluster files\n",
    "\n",
    "This assumes that your seurat object is saved as 'obj' from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# USER INPUT: change this if your dimensionality reduction info is not saved in @meta.data\n",
    "dim_red_dataframe <- obj@meta.data\n",
    "\n",
    "# USER INPUT: change these if the column names of your dimensionality reduction are not the ones used below\n",
    "X_name <- \"X_umap1\"\n",
    "Y_name <- \"X_umap2\"\n",
    "\n",
    "# USER INPUT: change this to reflect the name of your dimensionality reduction type\n",
    "cluster_file_prefix <- paste(output_prefix, proj_name, \"umap\", sep=\"_\")\n",
    "\n",
    "save_cluster_file(dim_red_dataframe, X_name, Y_name, proj_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seurat object step 3: Get the metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o merged_metadata \n",
    "\n",
    "# if your metadata is not in obj@meta.data, change this\n",
    "seurat_metadata <- obj@meta.data\n",
    "\n",
    "\n",
    "merged_metadata <- data.frame(CELLS=paste(proj_name, rownames(seurat_metadata), sep=\"_\"), seurat_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  : \n",
      "  line 2 did not have 27 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyquist/anaconda3/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  : \n",
      "  line 2 did not have 27 elements\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyquist/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (4,20,21,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# TODO get the metadata object from R\n",
    "mapping_options[\"cell level dataframe\"] = pd.read_csv(\"/Users/nyquist/Dropbox (MIT)/Shalek Lab (Team folder conflict)/Projects/Gates/NIH.Vaccine.Route.Comparison.2019/R_combined_metadata.txt\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IDLow', 'Naive', 'AE', 'IV', 'IDHigh'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_options[\"cell level dataframe\"][\"VaccineRoute\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping_options[\"cell level dataframe\"][\"celltype\"] = \"\"\n",
    "mapping_options[\"cell level dataframe\"].loc[mapping_options[\"cell level dataframe\"][\"Week25CellType2\"].isna(),\"celltype\"] = mapping_options[\"cell level dataframe\"].loc[mapping_options[\"cell level dataframe\"][\"Week25CellType2\"].isna(),\"Week13CellType\"]\n",
    "mapping_options[\"cell level dataframe\"].loc[mapping_options[\"cell level dataframe\"][\"Week13CellType\"].isna(),\"celltype\"] = mapping_options[\"cell level dataframe\"].loc[mapping_options[\"cell level dataframe\"][\"Week13CellType\"].isna(),\"Week25CellType2\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanpy Step 1: Dimensionality reduction 'cluster files'\n",
    "\n",
    "The SCP expects one file per dimensionality reduction visualization, so the following code chunk will save a file for each dimensionality reduction stored in the ``obsm`` attribute of your ``AnnData`` object. The ``print(adata)`` command above should show you the available dimensionality reduction values. \n",
    "\n",
    "\n",
    "The upload interface allows you to input a minimum and maximum for each axis of each dimensionailty reduction. This next code chunk will also print out those results so you can enter them in the upload interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min for X_pca X is -12.995701\n",
      "max for X_pca X is 56.850597\n",
      "min for X_pca Y is -12.995701\n",
      "max for X_pca Y is 56.850597\n",
      "min for X_umap X is -6.8291025920439345\n",
      "max for X_umap X is 9.507524406034713\n",
      "min for X_umap Y is -6.8291025920439345\n",
      "max for X_umap Y is 9.507524406034713\n"
     ]
    }
   ],
   "source": [
    "if len(anndata_path) > 0:\n",
    "    conv.save_cluster_dfs(adata, output_dir)\n",
    "    # TODO: save output file names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert merged_metadata to dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanpy Step 2: expression matrices\n",
    "\n",
    "Next you will save your expression matrix in the SCP format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(anndata_path) > 0:\n",
    "    out_df = conv.make_expression_df(adata)\n",
    "    out_df.to_csv(os.path.join(output_dir, \"expression_file.txt.gz\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Both Scanpy and Seurat: Set up metadata\n",
    "\n",
    "Now we will convert metadata in your object to the correct metadata format and naming scheme for Alexandria\n",
    "\n",
    "\n",
    "This includes 3 parts:\n",
    "\n",
    "* Global Metadata\n",
    "\n",
    "* Alexandria structured metadata to be mapped from your metadata tables\n",
    "\n",
    "* Units for numeric metadata\n",
    "\n",
    "* Unstructured metadata mapped from your metadata tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust this if you are not looking for cell level and are doing sample level instead\n",
    "cell_level_metadata = pd.DataFrame(index=mapping_options[\"cell level dataframe\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a: Starting with global metadata\n",
    "\n",
    "These metadata attributes must be the same for all of your columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attributes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Metadata: Select species for this data\n",
    "\n",
    "Type a search term in the drop down below (ex. 'homo' for human or 'mus' for mouse)\n",
    "\n",
    "then run the next code section and select the value from that drop-down\n",
    "\n",
    "If you do not see the value you were looking for and would like to search again, type in a new value in the text box and *rerun the code block that generates the species*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc6b09102a64d6cb79c3819ddbb8e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Species', description='Species:', placeholder='Species search')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a search term for the species in the box above then run the next notebook cell to see a dropdown of search results\n"
     ]
    }
   ],
   "source": [
    "s =widgets.Text(\n",
    "    value='Species',\n",
    "    placeholder='Species search',\n",
    "    description='Species:',\n",
    "    disabled=False\n",
    ")\n",
    "display(s)\n",
    "print(\"enter a search term for the species in the box above then run the next notebook cell to see a dropdown of search results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b71ade482c4ad5ad373b209d1c39ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='species', options=(('Theretra rhesus: ', 'NCBITaxon_1088310'), ('Polites rhesus: ', 'NCB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_for_dropdown, name_id_dict=uh.query_search_term('ncbitaxon',s.value)\n",
    "m=uh.choose_metadata_name_dropdown(list_for_dropdown,\"species\")\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you are happy with your selection above, run the code block below!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attributes['species']=m.value\n",
    "global_attributes['species__ontology_label'] = name_id_dict[m.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Metadata: Select library preparation protocol for this data\n",
    "\n",
    "Just select from the dropdown below then run the code block under it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this takes a second if your internet connection isn't super fast\n",
      "11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313eb6d956e34db1a03de7444a83ec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='experimental method', options=(('RARseq: Restriction site associated RNA sequencing', 'E…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select from dropdown above then run the next notebook cell. If you want to change the selection, you need to re-run the cell below\n"
     ]
    }
   ],
   "source": [
    "print(\"this takes a second if your internet connection isn't super fast\")\n",
    "list_for_dropdown, name_id_dict = uh.query_all_values_under_root(\"efo\",\"EFO_0001457\")\n",
    "exp_method_dpdn=uh.choose_metadata_name_dropdown(list_for_dropdown,\"experimental method\")\n",
    "display(exp_method_dpdn)\n",
    "print(\"select from dropdown above then run the next notebook cell. If you want to change the selection, you need to re-run the cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq-Well saved as library preparation protocol\n"
     ]
    }
   ],
   "source": [
    "global_attributes['library_preparation_protocol']=exp_method_dpdn.value\n",
    "global_attributes['library_preparation_protocol__ontology_label'] = name_id_dict[exp_method_dpdn.value]\n",
    "\n",
    "print(name_id_dict[exp_method_dpdn.value] +\" saved as library preparation protocol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly saving the cellID column\n",
    "for k,v in global_attributes.items():\n",
    "    cell_level_metadata[k] = v\n",
    "\n",
    "cell_level_metadata[\"CellID\"] = cell_level_metadata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in all the metadata in the convention\n",
    "#TODO: make this work with JSON, for now it is just a table copy and pasted from the google sheets\n",
    "metadata_info = pd.read_csv(\"metadata_name_type_info.tsv\",sep=\"\\t\",index_col=0)\n",
    "metadata_info[\"is unit\"] = [\"unit\" in i for i in metadata_info.index] # removing units from this so people are less confused\n",
    "# removing the \"label\" types because we will add those automatically\n",
    "available_metadata = metadata_info[~metadata_info[\"class\"].isin([\"unit_label\", \"ontology_label\"]) & ~metadata_info[\"is unit\"]].index\n",
    "\n",
    "# we already added species and library prep so drop those as well\n",
    "available_metadata=available_metadata.drop(\"species\")\n",
    "available_metadata=available_metadata.drop('library_preparation_protocol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Renaming metadata from your dataframe and files\n",
    "\n",
    "Now we will facilitate mapping any of your metadata to the Alexandria metadata convention.\n",
    "\n",
    "If there is required (or optional) metadata that you would like to add that is not already in one of your files AND is an ontology or controlled list metadata type AND follows the same pattern as some other metadata in your files (ex. sample level or donor level), you can do so by mapping the metadata from some unrelated metadata.\n",
    "\n",
    "For example, if you recorded the organ of each sample but did not save it as an explicit column, you can choose the sample column and map from that\n",
    "\n",
    "\n",
    "You should run this group of cells once per metadata source (as in sample level file, your cell level dataframe, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the workhorse cell of the metadata adder!\n",
    "# TODO: make the labels for the dropdowns show the whole text\n",
    "# TODO: add some indication that you should just go back to the top and re-run for multiple files\n",
    "# TODO: give indication of which metadata has been added\n",
    "# TODO: non-controlled list doesnt bring you back to the starting dropdown\n",
    "meta_addr = ma.MetadataAdder(mapping_options, available_metadata, metadata_info, cell_level_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_level_metadata = meta_attr.cell_level_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you have any more files to add, go back to _Step 3b_. If not go to the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Check that all required keys are mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_keys = metadata_info.loc[metadata_info[\"required\"]==\"Yes\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need to add a value for is_living before proceeding!\n",
      "You need to add a value for sample_type before proceeding!\n",
      "You need to add a value for disease__ontology_label before proceeding!\n",
      "You need to add a value for CellID before proceeding!\n"
     ]
    }
   ],
   "source": [
    "for k in required_keys:\n",
    "    if k not in cell_level_metadata.columns:\n",
    "        #default?\n",
    "        default_val = metadata_info.loc[k,\"default\"]\n",
    "        if type(default_val) is str: # this is sketchy but right now all the defaults are strings so whatevs\n",
    "            cell_level_metadata[k] = default_val\n",
    "        else:\n",
    "            print(\"You need to add a value for \"+k+\" before proceeding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add these required values you should go back to Step 3b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: All numeric metadata needs units, so now add units..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8bf8887bff4366a82bfec7ef517070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='organism_age__unit', options=((\"month: A time unit which is approximately equal to the l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a list of unit metadata that you have\n",
    "my_metadata_info = metadata_info.loc[cell_level_metadata.columns]\n",
    "unit_dropdowns = {}\n",
    "name_id_dicts = {}\n",
    "for dep_metadata in my_metadata_info.loc[~my_metadata_info[\"dependent\"].isna()].index:\n",
    "    m_name = metadata_info.loc[dep_metadata,\"dependent\"]\n",
    "    \n",
    "    if metadata_info.loc[m_name, \"class\"] == \"ontology\":\n",
    "        ont = metadata_info.loc[m_name, \"ontology\"].split(\"/\")[-1]\n",
    "        list_for_dropdown, name_id_dict = uh.query_all_values_under_root(ont,metadata_info.loc[m_name, \"ontology_root\"])\n",
    "        unit_dropdowns[m_name] = uh.choose_metadata_name_dropdown(list_for_dropdown,m_name)\n",
    "        name_id_dicts[m_name] = name_id_dict\n",
    "    else:\n",
    "        unit_dropdowns[m_name] = widgets.Text(\n",
    "                    value=\"type unit here\",\n",
    "                    placeholder='type unit here',\n",
    "                    description=m_name,\n",
    "                    disabled=False\n",
    "                )\n",
    "    \n",
    "for n,v in unit_dropdowns.items():\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_values = {}\n",
    "for n,v in unit_dropdowns.items():\n",
    "    cells_with_dep = cell_level_metadata.loc[~cell_level_metadata[metadata_info.loc[n,\"dependency\"]].isna()].index\n",
    "    val = v.value\n",
    "    if n in name_id_dicts:\n",
    "        cell_level_metadata.loc[cells_with_dep, n+\"__ontology_label\"] = name_id_dicts[n][val]\n",
    "    cell_level_metadata.loc[cells_with_dep,n] = val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13341, 17)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4447, 8)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: Add in any unstructured metadata that you want to add (and their types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here are all the metadata columns in your cell level files:\")\n",
    "\n",
    "print(mapping_options[\"cell level dataframe\"].columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dictionary below, replace the keys with column names in the choices printed above and replace the values with the type: group or numeric, of the column. Add as many as you would like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructured_metadata_types = {\"columnname 1\":\"numeric\",\"columnname 2\": \"group\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in unstructured_metadata_types:\n",
    "    cell_level_metadata[m] = mapping_options[\"cell level dataframe\"].loc[cell_level_metadata.index, m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the structured Alexandria metadata types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['string', 'number', 'boolean'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_info['alexandria type']=metadata_info['type'].map({\"string\":\"group\",\"number\":\"numeric\",\"boolean\":\"group\"})\n",
    "\n",
    "types_row = pd.DataFrame(index= [\"TYPE\"], columns = cell_level_metadata.columns)\n",
    "\n",
    "for column in cell_level_metadata.columns:\n",
    "    if column in metadata_info.index:\n",
    "        types_row.loc[\"TYPE\",column] = metadata_info.loc[column,'alexandria type']\n",
    "        \n",
    "    else:\n",
    "        types_row.loc[\"TYPE\", column] = unstructured_metadata_types[column]\n",
    "\n",
    "\n",
    "final_metadata_dataframe = pd.concatenate([types_row, cell_level_metadata])\n",
    "\n",
    "final_metadata_dataframe.index.name = \"CELL\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write file to Alexandria format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_dataframe.to_csv(prefix+\"structured_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a description of how to figure out how to upload these files and a like to SCP (and how to get it in the alexandria namespace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
